{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, Concatenate, MultiHeadAttention, LayerNormalization, Add\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\aaani\\OneDrive - Birmingham City University\\Postgrad\\Dissertation\\Data'\n",
    "historical = pd.read_csv(f'{data_path}/historical_cohort.csv.gz')\n",
    "contemporary = pd.read_csv(f'{data_path}/contemporary_cohort.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target variable\n",
    "target_variable = 'readmission_30d'\n",
    "\n",
    "# Separate features (X) and the target variable (y)\n",
    "X_historical = historical.drop(target_variable, axis=1)\n",
    "y_historical = historical[target_variable]\n",
    "\n",
    "X_contemporary = contemporary.drop(target_variable, axis=1)\n",
    "y_contemporary = contemporary[target_variable]\n",
    "\n",
    "# Display the shapes of the data to confirm everything is loaded correctly\n",
    "print(f\"Historical Features (X_historical) shape: {X_historical.shape}\")\n",
    "print(f\"Historical Target (y_historical) shape: {y_historical.shape}\")\n",
    "print(f\"Contemporary Features (X_contemporary) shape: {X_contemporary.shape}\")\n",
    "print(f\"Contemporary Target (y_contemporary) shape: {y_contemporary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical variable gender using one-hot encoding\n",
    "historical_x_encoded = pd.get_dummies(X_historical, columns=['gender'], prefix='gender', dtype=int)\n",
    "contemporary_x_encoded = pd.get_dummies(X_contemporary, columns=['gender'], prefix='gender', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  number encode 'race'\n",
    "race_encoder = LabelEncoder()\n",
    "race_encoder.fit(historical_x_encoded['race']) #learn unique race values from historical data\n",
    "#mapping learned data to both historical and contemporary datasets\n",
    "historical_x_encoded['race_encoded'] = race_encoder.transform(historical_x_encoded['race'])\n",
    "contemporary_x_encoded['race_encoded'] = race_encoder.transform(contemporary_x_encoded['race'])\n",
    "\n",
    "# number encode 'icd_code'\n",
    "icd_encoder = LabelEncoder()\n",
    "icd_encoder.fit(historical_x_encoded['icd_code'])\n",
    "n_icd_codes = len(icd_encoder.classes_)\n",
    "historical_x_encoded['icd_code_encoded'] = icd_encoder.transform(historical_x_encoded['icd_code'])\n",
    "\n",
    "icd_mapping = {code: idx for idx, code in enumerate(icd_encoder.classes_)}\n",
    "\n",
    "contemporary_x_encoded['icd_code_encoded'] = contemporary_x_encoded['icd_code'].map(icd_mapping).fillna(n_icd_codes).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_x_encoded = historical_x_encoded.drop(columns=['race', 'icd_code'])\n",
    "contemporary_x_encoded = contemporary_x_encoded.drop(columns=['race', 'icd_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of columns to exclude from scaling\n",
    "cols_to_exclude = [\n",
    "    'subject_id', 'hadm_id', 'stay_id', \n",
    "    'anchor_year_group', 'hospital_expire_flag', \n",
    "    'admitted_to_icu', 'gender_F', 'gender_M', \n",
    "    'race_encoded', 'icd_code_encoded', 'icd_version'\n",
    "]\n",
    "\n",
    "# Identify numerical columns to scale\n",
    "numerical_cols_to_scale = [\n",
    "    col for col in historical_x_encoded.select_dtypes(include=np.number).columns \n",
    "    if col not in cols_to_exclude\n",
    "]\n",
    "\n",
    "# Initialise the scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(historical_x_encoded[numerical_cols_to_scale]) #fit only on historical data only, to prevent data leakage\n",
    "\n",
    "# create scaled versions of the datasets\n",
    "historical_x_scaled = historical_x_encoded.copy()\n",
    "contemporary_x_scaled = contemporary_x_encoded.copy()\n",
    "\n",
    "\n",
    "historical_x_scaled[numerical_cols_to_scale] = scaler.transform(historical_x_encoded[numerical_cols_to_scale])\n",
    "contemporary_x_scaled[numerical_cols_to_scale] = scaler.transform(contemporary_x_encoded[numerical_cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop model from discriminating based on year group, we will drop this column after saving it separately for comparison later\n",
    "historical_year_group = historical_x_scaled['anchor_year_group']\n",
    "contemporary_year_group = contemporary_x_scaled['anchor_year_group']\n",
    "\n",
    "historical_x_scaled = historical_x_scaled.drop(columns=['anchor_year_group'])\n",
    "contemporary_x_scaled = contemporary_x_scaled.drop(columns=['anchor_year_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 99/1/1 split due to large dataset size and further testing on the contemporary dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(historical_x_scaled, y_historical, \n",
    "    test_size=0.01, \n",
    "    random_state=42,\n",
    "    stratify=y_historical # Stratify to ensure class balance in both train and test sets\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.0101, # 1% of original data\n",
    "    random_state=42,\n",
    "    stratify=y_train # Stratify to ensure class balance in both train and val sets\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training samples: {len(X_train)} (~98%)\")\n",
    "print(f\"Validation samples: {len(X_val)} (~1%)\")\n",
    "print(f\"Test samples: {len(X_test)} (~1%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalise the random undersampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "#apply undersampling only to the training data\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['race_encoded', 'icd_code_encoded']\n",
    "numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "X_train_cat = X_train[categorical_features].values\n",
    "X_train_num = X_train[numerical_features].values\n",
    "\n",
    "X_val_cat = X_val[categorical_features].values\n",
    "X_val_num = X_val[numerical_features].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + inputs) # Add & Norm\n",
    "\n",
    "    # Feed Forward Part\n",
    "    ff_output = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    ff_output = Dropout(dropout)(ff_output)\n",
    "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "    \n",
    "    return LayerNormalization(epsilon=1e-6)(x + ff_output) # Add & Norm\n",
    "\n",
    "categorical_input = Input(shape=(X_train_cat.shape[1],), name='categorical_input', dtype='int64')\n",
    "embeddings = []\n",
    "embedding_dim = 32\n",
    "num_transformer_blocks = 2\n",
    "num_heads = 4\n",
    "ff_dim = 32\n",
    "mlp_units = [128, 64]\n",
    "dropout_rate = 0.2\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    n_unique = pd.concat([X_train[feature], X_val[feature]]).nunique()\n",
    "    emb = Embedding(input_dim=n_unique, output_dim=embedding_dim)(categorical_input[:, i])\n",
    "    embeddings.append(emb)\n",
    "x_cat = Concatenate()(embeddings)\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x_cat = transformer_encoder(x_cat, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
    "x_cat = Flatten()(x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numerical = X_train.drop(columns=['race_encoded', 'icd_code_encoded']).values\n",
    "X_train_race = X_train['race_encoded'].values\n",
    "X_train_icd = X_train['icd_code_encoded'].values\n",
    "\n",
    "X_val_numerical = X_val.drop(columns=['race_encoded', 'icd_code_encoded']).values\n",
    "X_val_race = X_val['race_encoded'].values\n",
    "X_val_icd = X_val['icd_code_encoded'].values\n",
    "\n",
    "X_test_numerical = X_test.drop(columns=['race_encoded', 'icd_code_encoded']).values\n",
    "X_test_race = X_test['race_encoded'].values\n",
    "X_test_icd = X_test['icd_code_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_input_layer = Input(shape=(X_train_numerical.shape[1],), name='X_train_numerical')\n",
    "race_input_layer = Input(shape=(1,), name='X_train_race')\n",
    "icd_input_layer = Input(shape=(1,), name='X_train_icd')\n",
    "\n",
    "# This learns a dense vector for each race category\n",
    "n_unique_races = historical_x_encoded['race_encoded'].nunique()\n",
    "race_embedding = Embedding(input_dim=n_unique_races, output_dim=8, name='race_embedding')(race_input_layer)\n",
    "race_flat = Flatten()(race_embedding)\n",
    "\n",
    "# This learns a dense vector for each ICD code\n",
    "n_unique_icd = len(icd_encoder.classes_) + 1\n",
    "icd_embedding = Embedding(input_dim=n_unique_icd, output_dim=50, name='icd_embedding')(icd_input_layer)\n",
    "icd_flat = Flatten()(icd_embedding)\n",
    "\n",
    "concatenated_inputs = Concatenate()([numerical_input_layer, race_flat, icd_flat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(256, activation='relu')(concatenated_inputs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "# --- The Final Output Layer ---\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# --- Create and Compile the Final Model ---\n",
    "model = Model(inputs=[numerical_input_layer, race_input_layer, icd_input_layer], outputs=output)\n",
    "\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.AUC(name='auc')] # AUC is a great metric for this task\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_numerical, X_train_race, X_train_icd], y_train,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    validation_data=([X_val_numerical, X_val_race, X_val_icd], y_val),\n",
    "    #callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
